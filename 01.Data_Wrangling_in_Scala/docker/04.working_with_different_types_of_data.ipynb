{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://30cb468b60cf:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1598898596039)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@59828f8\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "    .appName(\"blabla\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;35m011.png\u001b[0m\r\n",
      "01.gentle_intro.ipynb\r\n",
      "01.gentle_intro_TEST.ipynb\r\n",
      "02.basic_structured_op.ipynb\r\n",
      "02.basic_structured_op_TEST.ipynb\r\n",
      "03.aggregations.ipynb\r\n",
      "04.working_with_different_types_of_data_INCOMPLETE.ipynb\r\n",
      "05.joins.ipynb\r\n",
      "05.joins_TEST.ipynb\r\n",
      "2010-12-01.csv\r\n",
      "201508_station_data.csv\r\n",
      "201508_trip_data.csv\r\n",
      "spark_docker.txt\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|    8/6/2013|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|San Jose|    8/7/2013|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [station_id: int, name: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"../../../src/201508_station_data.csv\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- dockcount: integer (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Spark Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  1|one|1.0|\n",
      "+---+---+---+\n",
      "|  1|one|1.0|\n",
      "|  1|one|1.0|\n",
      "+---+---+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lit(1), lit(\"one\"), lit(1.0)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 1: integer (nullable = false)\n",
      " |-- one: string (nullable = false)\n",
      " |-- 1.0: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lit(1), lit(\"one\"), lit(1.0)).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+---+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|one|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+---+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|  1|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|  1|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"one\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|San Jose|    8/7/2013|\n",
      "|         7|Paseo de San Antonio|37.333798|-121.886943|       15|San Jose|    8/7/2013|\n",
      "|         8| San Salvador at 1st|37.330165|-121.885831|       15|San Jose|    8/5/2013|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM df\n",
    "WHERE dockcount = 15\n",
    "\"\"\").show(4)\n",
    "\n",
    "df.where(df(\"dockcount\") === 15).selectExpr(\"*\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|    8/6/2013|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|San Jose|    8/7/2013|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM df\n",
    "WHERE dockcount <> 27\n",
    "\"\"\").show(4)\n",
    "\n",
    "df.where(col(\"dockcount\") =!= 27).selectExpr(\"*\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+------------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|    landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+------------+------------+\n",
      "|        21|   Franklin at Maple|37.481758|-122.226904|       15|Redwood City|   8/12/2013|\n",
      "|        22|Redwood City Calt...|37.486078|-122.232089|       25|Redwood City|   8/15/2013|\n",
      "|        23|San Mateo County ...|37.487616|-122.229951|       15|Redwood City|   8/15/2013|\n",
      "|        24|Redwood City Publ...|37.484219|-122.227424|       15|Redwood City|   8/12/2013|\n",
      "+----------+--------------------+---------+-----------+---------+------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM df\n",
    "WHERE landmark <> 'San Jose'\n",
    "\"\"\").show(4)\n",
    "\n",
    "df.where(\"landmark <> 'San Jose'\").selectExpr(\"*\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|     landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "|        41|     Clay at Battery|37.795001| -122.39997|       15|San Francisco|   8/19/2013|\n",
      "|        42|    Davis at Jackson| 37.79728|-122.398436|       15|San Francisco|   8/19/2013|\n",
      "|        45|Commercial at Mon...|37.794231|-122.402923|       15|San Francisco|   8/19/2013|\n",
      "|        46|Washington at Kea...|37.795425|-122.404767|       15|San Francisco|   8/19/2013|\n",
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM df\n",
    "WHERE landmark = 'San Francisco'\n",
    "\"\"\").show(4)\n",
    "\n",
    "df.where(\"landmark = 'San Francisco'\").selectExpr(\"*\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+------------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|    landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+------------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|    San Jose|    8/6/2013|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|    San Jose|    8/5/2013|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|    San Jose|    8/6/2013|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|    San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|    San Jose|    8/7/2013|\n",
      "|         7|Paseo de San Antonio|37.333798|-121.886943|       15|    San Jose|    8/7/2013|\n",
      "|         8| San Salvador at 1st|37.330165|-121.885831|       15|    San Jose|    8/5/2013|\n",
      "|         9|           Japantown|37.348742|-121.894715|       15|    San Jose|    8/5/2013|\n",
      "|        10|  San Jose City Hall|37.337391|-121.886995|       15|    San Jose|    8/6/2013|\n",
      "|        11|         MLK Library|37.335885| -121.88566|       19|    San Jose|    8/6/2013|\n",
      "|        12|SJSU 4th at San C...|37.332808|-121.883891|       19|    San Jose|    8/7/2013|\n",
      "|        13|       St James Park|37.339301|-121.889937|       15|    San Jose|    8/6/2013|\n",
      "|        14|Arena Green / SAP...|37.332692|-121.900084|       19|    San Jose|    8/5/2013|\n",
      "|        16|SJSU - San Salvad...|37.333955|-121.877349|       15|    San Jose|    8/7/2013|\n",
      "|        21|   Franklin at Maple|37.481758|-122.226904|       15|Redwood City|   8/12/2013|\n",
      "+----------+--------------------+---------+-----------+---------+------------+------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stationFilter: org.apache.spark.sql.Column = (station_id > 6)\n",
       "landmarkFilter: org.apache.spark.sql.Column = contains(landmark, San)\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stationFilter = col(\"station_id\") > 6\n",
    "val landmarkFilter = col(\"landmark\").contains(\"San\")\n",
    "\n",
    "df.where(stationFilter.or(landmarkFilter)).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|     landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "|         7|Paseo de San Antonio|37.333798|-121.886943|       15|     San Jose|    8/7/2013|\n",
      "|         8| San Salvador at 1st|37.330165|-121.885831|       15|     San Jose|    8/5/2013|\n",
      "|         9|           Japantown|37.348742|-121.894715|       15|     San Jose|    8/5/2013|\n",
      "|        10|  San Jose City Hall|37.337391|-121.886995|       15|     San Jose|    8/6/2013|\n",
      "|        11|         MLK Library|37.335885| -121.88566|       19|     San Jose|    8/6/2013|\n",
      "|        12|SJSU 4th at San C...|37.332808|-121.883891|       19|     San Jose|    8/7/2013|\n",
      "|        13|       St James Park|37.339301|-121.889937|       15|     San Jose|    8/6/2013|\n",
      "|        14|Arena Green / SAP...|37.332692|-121.900084|       19|     San Jose|    8/5/2013|\n",
      "|        16|SJSU - San Salvad...|37.333955|-121.877349|       15|     San Jose|    8/7/2013|\n",
      "|        41|     Clay at Battery|37.795001| -122.39997|       15|San Francisco|   8/19/2013|\n",
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(stationFilter).where(landmarkFilter).show(10)\n",
    "\n",
    "// same thing with\n",
    "df.where(stationFilter.and(landmarkFilter)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|  test|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013| 297.0|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013| 450.0|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|    8/6/2013| 737.0|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|San Jose|    8/5/2013|2432.0|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val test = (pow(col(\"station_id\"), 3) + 3) * col(\"dockcount\")\n",
    "df.select(expr(\"*\"), test.alias(\"test\")).show(4)\n",
    "\n",
    "df.selectExpr(\"*\", \"(POWER(station_id, 3) + 3) * dockcount as test\").show(4)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT *, ((POWER(station_id, 3) + 3) * dockcount) AS test\n",
    "FROM df \"\"\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+\n",
      "|station_id|                name|dockcount|\n",
      "+----------+--------------------+---------+\n",
      "|         2|San Jose Diridon ...|       27|\n",
      "|        61|     2nd at Townsend|       27|\n",
      "|        67|      Market at 10th|       27|\n",
      "+----------+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+--------------------+---------+\n",
      "|station_id|                name|dockcount|\n",
      "+----------+--------------------+---------+\n",
      "|         4|Santa Clara at Al...|       11|\n",
      "|        32|Castro Street and...|       11|\n",
      "|        35|University and Em...|       11|\n",
      "+----------+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"manyDock\", not(df(\"dockcount\").leq(25)))\n",
    "    .filter(\"manyDock\")\n",
    "    .select(\"station_id\", \"name\", \"dockcount\").show(3)\n",
    "\n",
    "df.withColumn(\"notManyDock\", expr(\"NOT dockcount >= 12\"))\n",
    "    .filter(\"notManyDock\")\n",
    "    .select(\"station_id\", \"name\", \"dockcount\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|lat_rounded|\n",
      "+-----------+\n",
      "|       37.0|\n",
      "|       37.0|\n",
      "|       37.0|\n",
      "+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT round(lat) AS lat_rounded\n",
    "FROM df \"\"\").show(3)\n",
    "\n",
    "df.select(round(col(\"lat\"), 2).as(\"lat_rounded\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|brounded_lat|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|        37.0|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|        37.0|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|    8/6/2013|        37.0|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"brounded_lat\", bround(df(\"lat\"))).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+-----------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|rounded_lat|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+-----------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|      37.33|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|      37.33|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|    8/6/2013|      37.33|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"rounded_lat\", round(col(\"lat\"), 2)).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__General stats__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+-------------------+-----------------+\n",
      "|summary|        station_id|                name|                lat|        dockcount|\n",
      "+-------+------------------+--------------------+-------------------+-----------------+\n",
      "|  count|                70|                  70|                 70|               70|\n",
      "|   mean|              43.0|                null|  37.59024338428572|17.65714285714286|\n",
      "| stddev|24.166091947189145|                null|0.20347253639672416|4.010441857493954|\n",
      "|    min|                 2|       2nd at Folsom|          37.329732|               11|\n",
      "|    max|                84|Yerba Buena Cente...|           37.80477|               27|\n",
      "+-------+------------------+--------------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"station_id\", \"name\", \"lat\", \"dockcount\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Correlation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res33: Double = 0.24015841145323474\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.corr(\"station_id\", \"dockcount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|corr(station_id, dockcount)|\n",
      "+---------------------------+\n",
      "|        0.24015841145323474|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(corr(\"station_id\", \"dockcount\")).show()\n",
    "\n",
    "spark.sql(\"SELECT corr(station_id, dockcount) FROM df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Crosstab__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [the official documentation](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=crosstab#pyspark.sql.DataFrame.crosstab)\n",
    "the crosstab method computes a pair-wise frequency table of the given columns. Also known as a contingency table. The number of distinct values for each column should be less than 1e4. At most 1e6 non-zero pair frequencies will be returned. The first column of each row will be the distinct values of col1 and the column names will be the distinct values of col2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "//df.stat.crosstab(\"lat\", \"long\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas [freqItems](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=freqitems#pyspark.sql.DataFrame.freqItems)\n",
    " let's you find frequent items for columns, possibly with false positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|       lat_freqItems|      long_freqItems|\n",
      "+--------------------+--------------------+\n",
      "|[37.80477, 37.798...|[-122.229951, -12...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.freqItems(Seq(\"lat\", \"long\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a unique ID to each row (note that the number doesn't necesseraly follow each other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----------+\n",
      "|monotonically_increasing_id()|station_id|\n",
      "+-----------------------------+----------+\n",
      "|                            0|         2|\n",
      "|                            1|         3|\n",
      "|                            2|         4|\n",
      "|                            3|         5|\n",
      "|                            4|         6|\n",
      "+-----------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(monotonically_increasing_id(), col(\"station_id\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+---------------------------------+---------------------------------+\n",
      "|lower(Name)                      |upper(Name)                      |initcap(lower(Name))             |\n",
      "+---------------------------------+---------------------------------+---------------------------------+\n",
      "|san jose diridon caltrain station|SAN JOSE DIRIDON CALTRAIN STATION|San Jose Diridon Caltrain Station|\n",
      "|san jose civic center            |SAN JOSE CIVIC CENTER            |San Jose Civic Center            |\n",
      "+---------------------------------+---------------------------------+---------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lower(col(\"Name\")), upper(col(\"Name\")), initcap(lower(col(\"Name\")))).show(2, false)\n",
    "\n",
    "spark.sql(\"SELECT lower(Name), upper(Name), initcap(lower(Name)) FROM df\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if lpad or rpad takes a number less than the length of the string, it will always remove values from the right side of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-----------------+------------------+\n",
      "|ltrim(   HELLLO   )|rtrim(   HELLLO   )|trim(    HELLLO   )|lpad(HELLO, 3,  )|rpad(HELLO, 10,  )|\n",
      "+-------------------+-------------------+-------------------+-----------------+------------------+\n",
      "|          HELLLO   |             HELLLO|             HELLLO|              HEL|        HELLO     |\n",
      "|          HELLLO   |             HELLLO|             HELLLO|              HEL|        HELLO     |\n",
      "+-------------------+-------------------+-------------------+-----------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    ltrim(lit(\"     HELLO     \")).as(\"ltrim\"),\n",
    "    rtrim(lit(\"     HELLO     \")).as(\"rtrim\"),\n",
    "    trim(lit(\"      HELLO     \")).as(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 3, \" \").as(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").as(\"rp\")).show(2)\n",
    "\n",
    "spark.sql(\"\"\"SELECT\n",
    "    ltrim('   HELLLO   '),\n",
    "    rtrim('   HELLLO   '),\n",
    "    trim('    HELLLO   '),\n",
    "    lpad('HELLO', 3, ' '),\n",
    "    rpad('HELLO', 10, ' ')\n",
    "FROM df\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Regular Expressions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two key functions in Spark that you’ll need in order to perform regular expression tasks: regexp_extract and regexp_replace. These functions extract values and replace values, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace substitute specific names in our name column with TEST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          name_clean|                name|\n",
      "+--------------------+--------------------+\n",
      "|TEST JOSE DIRIDON...|SAN JOSE DIRIDON ...|\n",
      "|TEST JOSE CIVIC C...|SAN JOSE CIVIC CE...|\n",
      "|TESTTA CLARA AT A...|SANTA CLARA AT AL...|\n",
      "|     TEST ON ALMADEN|    ADOBE ON ALMADEN|\n",
      "|   TEST PEDRO SQUARE|    SAN PEDRO SQUARE|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfBis: org.apache.spark.sql.DataFrame = [station_id: int, name: string ... 5 more fields]\n",
       "import org.apache.spark.sql.functions.regexp_replace\n",
       "simpleWords: Seq[String] = List(san, santa, adobe)\n",
       "regexString: String = SAN|SANTA|ADOBE\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfBis = df.withColumn(\"name\", upper(col(\"name\")))\n",
    "\n",
    "import org.apache.spark.sql.functions.regexp_replace\n",
    "\n",
    "val simpleWords = Seq(\"san\", \"santa\", \"adobe\")\n",
    "val regexString = simpleWords.map(_.toUpperCase).mkString(\"|\")\n",
    "// the | signifies `OR` in regular expression syntax\n",
    "\n",
    "dfBis.select(\n",
    "    regexp_replace(col(\"name\"), regexString, \"TEST\").alias(\"name_clean\"),\n",
    "    col(\"name\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace given characters with other characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+--------+\n",
      "|translate(landmark, San, S4N)|landmark|\n",
      "+-----------------------------+--------+\n",
      "|                     S4N Jose|San Jose|\n",
      "|                     S4N Jose|San Jose|\n",
      "+-----------------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(translate(col(\"landmark\"), \"San\", \"S4N\"), col(\"landmark\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling out the first mentioned word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|name_clean|                name|\n",
      "+----------+--------------------+\n",
      "|       SAN|SAN JOSE DIRIDON ...|\n",
      "|       SAN|SAN JOSE CIVIC CE...|\n",
      "+----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.regexp_extract\n",
       "regexString: String = (SAN|SANTA|ADOBE)\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.regexp_extract\n",
    "val regexString = simpleWords.map(_.toUpperCase).mkString(\"(\", \"|\", \")\")\n",
    "// the | signifies OR in regular expression syntax\n",
    "dfBis.select(\n",
    "    regexp_extract(col(\"name\"), regexString, 1).alias(\"name_clean\"),\n",
    "    col(\"name\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for their existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|SAN JOSE DIRIDON ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|\n",
      "|         3|SAN JOSE CIVIC CE...|37.330698|-121.888979|       15|San Jose|    8/5/2013|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfBis.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+--------+\n",
      "|name                             |landmark|\n",
      "+---------------------------------+--------+\n",
      "|SAN JOSE DIRIDON CALTRAIN STATION|San Jose|\n",
      "|SAN JOSE CIVIC CENTER            |San Jose|\n",
      "|SANTA CLARA AT ALMADEN           |San Jose|\n",
      "+---------------------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "containsSan: org.apache.spark.sql.Column = contains(name, SAN)\n",
       "containsJose: org.apache.spark.sql.Column = contains(landmark, Jose)\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val containsSan = col(\"name\").contains(\"SAN\")\n",
    "val containsJose = col(\"landmark\").contains(\"Jose\")\n",
    "\n",
    "dfBis.withColumn(\"test\", containsSan.or(containsJose))\n",
    "    .where(\"test\")\n",
    "    .select(\"name\", \"landmark\").show(3, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+\n",
      "| id|current_date|   current_timestamp|\n",
      "+---+------------+--------------------+\n",
      "|  0|  2020-08-29|2020-08-29 15:58:...|\n",
      "|  1|  2020-08-29|2020-08-29 15:58:...|\n",
      "|  2|  2020-08-29|2020-08-29 15:58:...|\n",
      "+---+------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- current_date: date (nullable = false)\n",
      " |-- current_timestamp: timestamp (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dateDF: org.apache.spark.sql.DataFrame = [id: bigint, current_date: date ... 1 more field]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dateDF = spark.range(3)\n",
    "    .withColumn(\"current_date\", current_date())\n",
    "    .withColumn(\"current_timestamp\", current_timestamp())\n",
    "\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "dateDF.show()\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------+\n",
      "|date_sub(current_date, 5)|date_add(current_date, 5)|\n",
      "+-------------------------+-------------------------+\n",
      "|               2020-08-24|               2020-09-03|\n",
      "+-------------------------+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(date_sub(col(\"current_date\"), 5), date_add(col(\"current_date\"), 5)).show(1)\n",
    "spark.sql(\"SELECT date_sub(current_date, 5), date_add(current_date, 5) FROM dateTable\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|datediff(week_ago, current_date)|\n",
      "+--------------------------------+\n",
      "|                              -7|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"current_date\"), 7))\n",
    "    .select(datediff(col(\"week_ago\"), col(\"current_date\"))).show(1)\n",
    "\n",
    "dateDF.select(\n",
    "    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "    to_date(lit(\"2017-05-22\")).alias(\"end\"))\n",
    "    .select(months_between(col(\"start\"), col(\"end\"))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date doesn't require a format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2017-01-01|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\n",
    "    .select(to_date(col(\"date\"))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The to_date function allows you to convert a string to a date, optionally with a specified format. Spark will not throw an error if it cannot parse the date; rather, it will just return null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dateFormat: String = yyyy-dd-MM\n",
       "cleanDateDF: org.apache.spark.sql.DataFrame = [date: date, date2: date]\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dateFormat = \"yyyy-dd-MM\"\n",
    "val cleanDateDF = spark.range(1).select(\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n",
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|               date|              date2|\n",
      "+-------------------+-------------------+\n",
      "|2017-11-12 00:00:00|2017-12-20 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- to_timestamp(`date`, 'yyyy-dd-MM'): timestamp (nullable = true)\n",
      "\n",
      "+---------------------------------------------+----------------------------------------------+\n",
      "|to_timestamp(datetable2.`date`, 'yyyy-dd-MM')|to_timestamp(datetable2.`date2`, 'yyyy-dd-MM')|\n",
      "+---------------------------------------------+----------------------------------------------+\n",
      "|                          2017-11-12 00:00:00|                           2017-12-20 00:00:00|\n",
      "+---------------------------------------------+----------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cleanDF: org.apache.spark.sql.DataFrame = [date: timestamp, date2: timestamp]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cleanDF = cleanDateDF\n",
    "    .withColumn(\"date\", to_timestamp(col(\"date\"), dateFormat))\n",
    "    .withColumn(\"date2\", to_timestamp(col(\"date2\"), dateFormat))\n",
    "                \n",
    "cleanDF.show()\n",
    "cleanDF.select(to_timestamp(col(\"date\"), dateFormat)).printSchema()\n",
    "\n",
    "spark.sql(\"\"\"SELECT to_timestamp(date, 'yyyy-dd-MM'), to_timestamp(date2, 'yyyy-dd-MM')\n",
    "FROM dateTable2\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|current_date|\n",
      "+---+------------+\n",
      "|  0|  2020-08-29|\n",
      "+---+------------+\n",
      "\n",
      "+---+------------+\n",
      "| id|current_date|\n",
      "+---+------------+\n",
      "+---+------------+\n",
      "\n",
      "+---+------------+\n",
      "| id|current_date|\n",
      "+---+------------+\n",
      "|  0|  2020-08-29|\n",
      "+---+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dateDF: org.apache.spark.sql.DataFrame = [id: bigint, current_date: date]\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dateDF = spark.range(1).withColumn(\"current_date\", current_date())\n",
    "dateDF.show()\n",
    "\n",
    "dateDF.filter(col(\"current_date\") <= lit(\"2017-12-12\")).show()\n",
    "dateDF.filter(col(\"current_date\") >= lit(\"2017-12-12\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+----------+\n",
      "| id|   unix_ts|         time_stamp|      date|\n",
      "+---+----------+-------------------+----------+\n",
      "|  0|1598722033|2020-08-29 17:27:13|2020-08-29|\n",
      "+---+----------+-------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dateDF: org.apache.spark.sql.DataFrame = [id: bigint, unix_ts: bigint ... 2 more fields]\n"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dateDF = spark.range(1)\n",
    "    .withColumn(\"unix_ts\", unix_timestamp(current_timestamp, \"MM:dd:yyyy hh:mm:ss\"))\n",
    "    .withColumn(\"time_stamp\", col(\"unix_ts\").cast(\"timestamp\")) // or to_timestamp  \n",
    "    .withColumn(\"date\", to_date(col(\"time_stamp\")))\n",
    "\n",
    "dateDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Nulls in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Coalesce__\n",
    "The coalesce function allows you to select the first non-null value from a set of columns If there are no null values, so it simply returns the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"../../../src/2010-12-01.csv\")\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(\"columnName\")            // On a specific DataFrame.\n",
    "col(\"columnName\")           // A generic column no yet associated with a DataFrame.\n",
    "col(\"columnName.field\")     // Extracting a struct field\n",
    "col(\"`a.column.with.dots`\") // Escape `.` in column names.\n",
    "$\"columnName\"               // Scala short hand for a named column.\n",
    "expr(\"a + 1\")               // A column that is constructed from a parsed SQL Expression.\n",
    "lit(\"abc\")                  // A column that produces a literal (constant) value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|coalesce(StockCode, CustomerID)|\n",
      "+-------------------------------+\n",
      "|                         85123A|\n",
      "|                          71053|\n",
      "|                         84406B|\n",
      "+-------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(coalesce(col(\"StockCode\"), df(\"CustomerID\"))).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "//not working examples\n",
    "//df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()\n",
    "//df.select(for (c <- df.columns) yield count(when(isnan(c), c)).alias(c)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nb of NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "println(df.filter(df(\"Description\").isNull).count)\n",
    "println(df.filter(df(\"Description\") === \"\").count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 10\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df(\"Description\").isNull || df(\"Description\") === \"\").count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(InvoiceNo,10)\n",
      "(StockCode,10)\n",
      "(Description,10)\n",
      "(Quantity,10)\n",
      "(InvoiceDate,10)\n",
      "(UnitPrice,10)\n",
      "(CustomerID,10)\n",
      "(Country,10)\n"
     ]
    }
   ],
   "source": [
    "for (c <- df.columns) println(c, df.filter(df(\"Description\").isNull || df(\"Description\") === \"\").count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to find duplicated lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|count|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|   536407|    22632|HAND WARMER RED P...|       6|2010-12-01 11:34:00|     1.85|   17850.0|United Kingdom|    1|\n",
      "|   536408|   84029E|RED WOOLLY HOTTIE...|       4|2010-12-01 11:41:00|     3.75|   14307.0|United Kingdom|    1|\n",
      "|   536409|    22531|MAGIC DRAWING SLA...|       1|2010-12-01 11:45:00|     0.42|   17908.0|United Kingdom|    1|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\", \"CustomerID\", \"Country\")\n",
    "    .count().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df.groupBy(df.columns)\\\n",
    "    .count()\\\n",
    "    .where(f.col('count') > 1)\\\n",
    "    .select(f.sum('count'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3064,3108)\n"
     ]
    }
   ],
   "source": [
    "println(df.distinct().count(), df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfB: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, StockCode: string ... 6 more fields]\n",
       "res10: Long = 3064\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dfB = df.dropDuplicates()\n",
    "dfB.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows that contain nulls : the default is to drop any row in which any value is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Long = 1924\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfB.na.drop(\"any\").count() // or drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Long = 3064\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfB.na.drop(\"all\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill all null values in columns of type String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.fill(\"All Null values become this string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.fill(5, Seq(\"StockCode\", \"InvoiceNo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fillColValues: scala.collection.immutable.Map[String,Any] = Map(StockCode -> 5, Description -> No Value)\n",
       "res18: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fillColValues = Map(\"StockCode\" -> 5, \"Description\" -> \"No Value\")\n",
    "df.na.fill(fillColValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more flexible option that you can use with more than just null values. (only requirement is that this value be\n",
    "the same type as the original value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.replace(\"Description\", Map(\"\" -> \"UNKNOWN\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
