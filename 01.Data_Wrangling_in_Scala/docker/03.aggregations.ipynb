{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://858e3ef863ab:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1596284414786)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@965f5e2\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "    .appName(\"blabla\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.gentle_intro_COMPLEMENT.ipynb\r\n",
      "01.gentle_intro.ipynb\r\n",
      "01.gentle_intro_TEST.ipynb\r\n",
      "02.basic_structured_op_COMPLEMENT.ipynb\r\n",
      "02.basic_structured_op.ipynb\r\n",
      "02.basic_structured_op_TEST.ipynb\r\n",
      "03.aggregations_INCOMPLETE.ipynb\r\n",
      "03.aggregations.ipynb\r\n",
      "04.working_with_different_types_of_data.ipynb\r\n",
      "05.joins.ipynb\r\n",
      "201508_station_data.csv\r\n",
      "201508_trip_data.csv\r\n",
      "spark_docker.txt\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [station_id: string, name: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"inferScema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"../../../src/201508_station_data.csv\")\n",
    "\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the nb of lines, of columns and the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70,7)\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- long: string (nullable = true)\n",
      " |-- dockcount: string (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(df.count(), df.columns.size)\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change type of various cols (lat & long to long, dockcount & station_id to int and installation to date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: long (nullable = true)\n",
      " |-- long: long (nullable = true)\n",
      " |-- dockcount: integer (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "df2: org.apache.spark.sql.DataFrame = [station_id: int, name: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val df2 = df.withColumn(\"station_id\", col(\"station_id\").cast(\"int\"))\n",
    "    .withColumn(\"lat\", col(\"lat\").cast(\"long\"))\n",
    "    .withColumn(\"long\", col(\"long\").cast(LongType))\n",
    "    .withColumn(\"dockcount\", col(\"dockcount\").cast(\"int\"))\n",
    "    .withColumn(\"installation\", col(\"installation\").cast(DateType))\n",
    "\n",
    "df2.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val df4 = df.withColumn(\"station_id\", col(\"station_id\").cast(\"int\")).select(\"*\")\n",
    "will work if you want to change only one col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative way with selectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: long (nullable = true)\n",
      " |-- long: long (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: date (nullable = true)\n",
      " |-- dockcount_long: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df3: org.apache.spark.sql.DataFrame = [station_id: int, name: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df3 = df2.selectExpr(\"*\", \"cast(dockcount as long) dockcount_long\").drop(\"dockcount\")\n",
    "\n",
    "df3.printSchema                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the nb of lines for the column \"landmark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|count(landmark)|\n",
      "+---------------+\n",
      "|             70|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(count(\"landmark\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count distinct values of the landmark column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT landmark)|\n",
      "+------------------------+\n",
      "|                       5|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(countDistinct(\"landmark\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing but without returning a DF, but directly the result as a nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Long = 5\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.select(\"landmark\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same thing with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT landmark)|\n",
      "+------------------------+\n",
      "|                       5|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "df2.createOrReplaceTempView(\"dfTable\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(distinct(landmark))\n",
    "FROM dfTable\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count approximatively distinct values of the landmark column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|approx_count_distinct(landmark)|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(approx_count_distinct(\"landmark\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, it can also be done in an SQL fashion (the 2nd param can be omitted : the error is equal to 0.05 by defaut :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|approx_count_distinct(landmark)|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT approx_count_distinct(landmark, 0.1) \n",
    "FROM dfTable\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's retrieve the first and last elements of a specific column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------+\n",
      "|first(landmark, false)|last(name, false)|\n",
      "+----------------------+-----------------+\n",
      "|              San Jose|      Ryland Park|\n",
      "+----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(first(\"landmark\"), last(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with an SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------+\n",
      "|first(landmark, false)|last(name, false)|\n",
      "+----------------------+-----------------+\n",
      "|              San Jose|      Ryland Park|\n",
      "+----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT first(landmark), last(name)\n",
    "FROM dfTable\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "// https://sparkbyexamples.com/spark/spark-change-dataframe-column-type/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also be interesting in learning the minimum & maximum of a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|min(station_id)|max(station_id)|\n",
      "+---------------+---------------+\n",
      "|              2|             84|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(min(\"station_id\"), max(\"station_id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|min(station_id)|max(station_id)|\n",
      "+---------------+---------------+\n",
      "|              2|             84|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT min(station_id), max(station_id)\n",
    "FROM dfTable\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum, min & avg of one or more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+--------+\n",
      "|sum(dockcount)|min(long)|avg(lat)|\n",
      "+--------------+---------+--------+\n",
      "|          1236|     -122|    37.0|\n",
      "+--------------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(sum(\"dockcount\"), min(\"long\"), avg(\"lat\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+--------+\n",
      "|min(dockcount)|min(long)|avg(lat)|\n",
      "+--------------+---------+--------+\n",
      "|            11|     -122|    37.0|\n",
      "+--------------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT min(dockcount), min(long), avg(lat)\n",
    "FROM dfTable\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the sum of all values with the sum of the distinct values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------+\n",
      "|sum(DISTINCT dockcount)|sum(dockcount)|\n",
      "+-----------------------+--------------+\n",
      "|                    120|          1236|\n",
      "+-----------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(sumDistinct(\"dockcount\"), sum(\"dockcount\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always it can be also calculated with a SQL statement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------+\n",
      "|sum(DISTINCT dockcount)|sum(dockcount)|\n",
      "+-----------------------+--------------+\n",
      "|                    120|          1236|\n",
      "+-----------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT sum(distinct(dockcount)), sum(dockcount)\n",
    "FROM dfTable\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avg calculated with selectExpr (side note : it'll fail with only one \"selectExpr\") :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+---+-----------------+\n",
      "|              AVG|TOTAL| NB|   CALCULATED_AVG|\n",
      "+-----------------+-----+---+-----------------+\n",
      "|17.65714285714286| 1236| 70|17.65714285714286|\n",
      "+-----------------+-----+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.selectExpr(\"avg(dockcount) as AVG\",\n",
    "               \"sum(dockcount) as TOTAL\",\n",
    "               \"count(dockcount) as NB\").selectExpr(\"*\", \"TOTAL/NB as CALCULATED_AVG\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with a nested SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+---+-----------------+\n",
      "|             avg_| tot| nb|   calculated_avg|\n",
      "+-----------------+----+---+-----------------+\n",
      "|17.65714285714286|1236| 70|17.65714285714286|\n",
      "+-----------------+----+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *, tot/nb as calculated_avg\n",
    "FROM (\n",
    "    SELECT avg(dockcount) AS avg_, sum(dockcount) AS tot, count(dockcount) as nb\n",
    "    FROM dfTable\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variance & standard deviation :__\n",
    "\n",
    "The variance is the average of the squared differences from the mean, and the standard deviation is the square root of the variance. You can calculate these in Spark by using their respective functions. \n",
    "\n",
    "Spark has both the formula for the sample standard deviation as well as the formula for the population standard deviation. These are fundamentally different statistical formulae, and we need to differentiate between them. By default, Spark performs the formula for the sample standard deviation or variance if you use the variance or stddev functions.\n",
    "\n",
    "You can also specify these explicitly or refer to the population standard deviation or variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------------+\n",
      "|stddev_samp(dockcount)|var_samp(dockcount)|\n",
      "+----------------------+-------------------+\n",
      "|     4.010441857493954| 16.083643892339555|\n",
      "+----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(stddev(\"dockcount\"), variance(\"dockcount\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+---------------------+----------------------+\n",
      "|var_pop(dockcount)|var_samp(dockcount)|stddev_pop(dockcount)|stddev_samp(dockcount)|\n",
      "+------------------+-------------------+---------------------+----------------------+\n",
      "| 15.85387755102042| 16.083643892339555|    3.981692799679606|     4.010441857493954|\n",
      "+------------------+-------------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(var_pop(\"dockcount\"), var_samp(\"dockcount\"), stddev_pop(\"dockcount\"), stddev_samp(\"dockcount\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Skewness & kurtosis__\n",
    "\n",
    "A fundamental task in many statistical analyses is to characterize the location and variability of a data set. A further characterization of the data includes skewness and kurtosis.\n",
    "Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n",
    "\n",
    "Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case.\n",
    "\n",
    "source [Engineering Statistics Handbook](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm#:~:text=Skewness%20is%20a%20measure%20of,relative%20to%20a%20normal%20distribution.)\n",
    "\n",
    "An other interseting blog post on medium :\n",
    "https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa?gi=cefd32e14a26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|skewness(dockcount)| kurtosis(dockcount)|\n",
      "+-------------------+--------------------+\n",
      "| 0.7369248035176993|-0.13950371148566587|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(skewness(\"dockcount\"), kurtosis(\"dockcount\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|skewness(station_id)|kurtosis(station_id)|\n",
      "+--------------------+--------------------+\n",
      "|-0.06719168007171215| -1.1955050641926341|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(skewness(\"station_id\"), kurtosis(\"station_id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|corr(station_id, dockcount)|\n",
      "+---------------------------+\n",
      "|        0.24015841145323474|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(corr(\"station_id\", \"dockcount\")).show()\n",
    "// sidenote: it doesn't make sense to use the correlation with the station_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance\n",
    "\n",
    "In probability theory and statistics, the mathematical concepts of covariance and correlation are very similar.[1][2] Both describe the degree to which two random variables or sets of random variables tend to deviate from their expected values in similar ways. \n",
    "\n",
    "Source [Wikipedia](https://en.wikipedia.org/wiki/Covariance_and_correlation) \n",
    "\n",
    " “Covariance” indicates the direction of the linear relationship between variables. “Correlation” on the other hand measures both the strength and direction of the linear relationship between two variables. Correlation is a function of the covariance. What sets them apart is the fact that correlation values are standardized whereas, covariance values are not. You can obtain the correlation coefficient of two variables by dividing the covariance of these variables by the product of the standard deviations of the same values. If we revisit the definition of Standard Deviation, it essentially measures the absolute variability of a datasets’ distribution. When you divide the covariance values by the standard deviation, it essentially scales the value down to a limited range of -1 to +1. This is precisely the range of the correlation values.\n",
    " \n",
    "Source [Towards Datascience](https://towardsdatascience.com/let-us-understand-the-correlation-matrix-and-covariance-matrix-d42e6b643c22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|covar_pop(station_id, dockcount)|\n",
      "+--------------------------------+\n",
      "|              22.942857142857182|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(covar_pop(\"station_id\", \"dockcount\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------------------------+\n",
      "|covar_pop(station_id, dockcount)|covar_samp(station_id, dockcount)|\n",
      "+--------------------------------+---------------------------------+\n",
      "|              22.942857142857182|                23.27536231884062|\n",
      "+--------------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(covar_pop(\"station_id\", \"dockcount\"), covar_samp(\"station_id\", \"dockcount\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating to Complex Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve a set and a list of all values of a specific column :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------+\n",
      "|collect_set(landmark)|collect_list(landmark)|\n",
      "+---------------------+----------------------+\n",
      "| [San Jose, San Fr...|  [San Jose, San Jo...|\n",
      "+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.agg(collect_set(\"landmark\"), collect_list(\"landmark\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what is the first element of both two results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res38: org.apache.spark.sql.Row = [WrappedArray(San Jose, San Francisco, Palo Alto, Redwood City, Mountain View)]\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.agg(collect_set(\"landmark\")).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res45: Any = WrappedArray(San Jose, San Jose, San Jose, San Jose, San Jose, San Jose, San Jose, San Jose, San Jose, San Jose, San Jose, San Jose, San Jose, San Jose, Redwood City, Redwood City, Redwood City, Redwood City, Redwood City, Redwood City, Mountain View, Mountain View, Mountain View, Mountain View, Mountain View, Mountain View, Mountain View, Palo Alto, Palo Alto, Palo Alto, Palo Alto, Palo Alto, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisco, San Francisc...\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.agg(collect_list(\"landmark\")).first()(0)\n",
    "// (0) otherwise one get the Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use expression to calculate the sum of dockcount for each group of landmark, and sort by descending order the results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|     landmark|sum(dockcount)|\n",
      "+-------------+--------------+\n",
      "|San Francisco|           665|\n",
      "|     San Jose|           264|\n",
      "|Mountain View|           117|\n",
      "| Redwood City|           115|\n",
      "|    Palo Alto|            75|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy(\"landmark\").agg(sum(\"dockcount\")).sort(desc(\"sum(dockcount)\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing by grouping with two cols : landmark & name :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------+\n",
      "|     landmark|                name|sum(dockcount)|\n",
      "+-------------+--------------------+--------------+\n",
      "|San Francisco|   Market at Sansome|            27|\n",
      "|San Francisco|      Market at 10th|            27|\n",
      "|     San Jose|San Jose Diridon ...|            27|\n",
      "|San Francisco|     2nd at Townsend|            27|\n",
      "| Redwood City|Redwood City Calt...|            25|\n",
      "|San Francisco| Golden Gate at Polk|            23|\n",
      "|San Francisco|   Steuart at Market|            23|\n",
      "|Mountain View|Mountain View Cal...|            23|\n",
      "|    Palo Alto|Palo Alto Caltrai...|            23|\n",
      "|Mountain View|San Antonio Caltr...|            23|\n",
      "+-------------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy(\"landmark\", \"name\").agg(sum(\"dockcount\")).orderBy(desc(\"sum(dockcount)\")).show(10)\n",
    "/*\n",
    "df2.groupBy(\"landmark\", \"name\").agg(sum(\"dockcount\")).sort(desc(\"sum(dockcount)\")).show(10)\n",
    "same thing with sort but in SQL only ORDER BY works ! */"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----+\n",
      "|     landmark|                name|Total|\n",
      "+-------------+--------------------+-----+\n",
      "|San Francisco|     2nd at Townsend|   27|\n",
      "|San Francisco|   Market at Sansome|   27|\n",
      "|     San Jose|San Jose Diridon ...|   27|\n",
      "|San Francisco|      Market at 10th|   27|\n",
      "| Redwood City|Redwood City Calt...|   25|\n",
      "|San Francisco|Harry Bridges Pla...|   23|\n",
      "|Mountain View|Mountain View Cal...|   23|\n",
      "|Mountain View|San Antonio Caltr...|   23|\n",
      "|San Francisco|   Steuart at Market|   23|\n",
      "|    Palo Alto|Palo Alto Caltrai...|   23|\n",
      "+-------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT landmark, name, sum(dockcount) as Total\n",
    "FROM dfTable\n",
    "GROUP BY landmark, name\n",
    "ORDER BY total DESC\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An other way to make group is by using maps :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+---------------------+\n",
      "|     landmark|    avg(dockcount)|stddev_pop(dockcount)|\n",
      "+-------------+------------------+---------------------+\n",
      "|    Palo Alto|              15.0|    4.381780460041329|\n",
      "|San Francisco|              19.0|    3.703280399090206|\n",
      "|     San Jose|              16.5|    3.427827300200522|\n",
      "| Redwood City|16.428571428571427|   3.4992710611188254|\n",
      "|Mountain View|16.714285714285715|    4.199125273342591|\n",
      "+-------------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"landmark\").agg(\"dockcount\" -> \"avg\", \"dockcount\" -> \"stddev_pop\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "simpleData: Seq[(String, String, Int)] = List((James,Sales,3000), (Michael,Sales,4600), (Robert,Sales,4100), (Maria,Finance,3000), (James,Sales,3000), (Scott,Finance,3300), (Jen,Finance,3900), (Jeff,Marketing,3000), (Kumar,Marketing,2000), (Saif,Sales,4100))\n",
       "df: org.apache.spark.sql.DataFrame = [employee_name: string, department: string ... 1 more field]\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val simpleData = Seq((\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  )\n",
    "\n",
    "val df = simpleData.toDF(\"employee_name\", \"department\", \"salary\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "row_number() window function is used to give the sequential row number starting from 1 to the result of each window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         2|\n",
      "|       Robert|     Sales|  4100|         3|\n",
      "|         Saif|     Sales|  4100|         4|\n",
      "|      Michael|     Sales|  4600|         5|\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@459ce8d3\n"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an aggregation across multiple groups. We achieve this by using grouping sets. Grouping\n",
    "sets are a low-level tool for combining sets of aggregations together. They give you the ability to\n",
    "create arbitrary aggregation in their group-by statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : https://mungingdata.com/apache-spark/aggregations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cube__\n",
    "\n",
    "A cube takes the rollup to a level deeper. Rather than treating elements hierarchically, a cube does the same thing across all dimensions. This means that it won’t just go by column over the entire possibilities, but also the other cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|word|num|\n",
      "+----+---+\n",
      "| bar|  2|\n",
      "| bar|  2|\n",
      "| foo|  1|\n",
      "| foo|  2|\n",
      "+----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [word: string, num: bigint]\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "  (\"bar\", 2L),\n",
    "  (\"bar\", 2L),\n",
    "  (\"foo\", 1L),\n",
    "  (\"foo\", 2L)\n",
    ").toDF(\"word\", \"num\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+\n",
      "|word| num|count(num)|\n",
      "+----+----+----------+\n",
      "| bar|   2|         2|\n",
      "|null|null|         4|\n",
      "| foo|   2|         1|\n",
      "|null|   1|         1|\n",
      "| foo|null|         2|\n",
      "| foo|   1|         1|\n",
      "|null|   2|         3|\n",
      "| bar|null|         2|\n",
      "+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cube(\"word\", \"num\").agg(count(\"num\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+----+----+-----+\n",
    "|word| num|count|\n",
    "+----+----+-----+\n",
    "|null|null|    4| Total rows in df\n",
    "|null|   1|    1| Count where num equals 1\n",
    "|null|   2|    3| Count where num equals 2\n",
    "| bar|null|    2| Where word equals bar\n",
    "| bar|   2|    2| Where word equals bar and num equals 2\n",
    "| foo|null|    2| Where word equals foo\n",
    "| foo|   1|    1| Where word equals foo and num equals 1\n",
    "| foo|   2|    1| Where word equals foo and num equals 2\n",
    "+----+----+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rollups__\n",
    "\n",
    "A rollup is a multidimensional aggregation that performs a variety of group-by style calculations for us. It's a subset of cube that “computes hierarchical subtotals from left to right”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|word| num|count|\n",
      "+----+----+-----+\n",
      "|null|null|    4|\n",
      "| bar|null|    2|\n",
      "| bar|   2|    2|\n",
      "| foo|null|    2|\n",
      "| foo|   1|    1|\n",
      "| foo|   2|    1|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.rollup(\"word\", \"num\")\n",
    "  .count()\n",
    "  .sort(asc(\"word\"), asc(\"num\"))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+----+----+-----+\n",
    "|word| num|count|\n",
    "+----+----+-----+\n",
    "|null|null|    4| Count of all rows\n",
    "| bar|null|    2| Count when word is bar\n",
    "| bar|   2|    2| Count when num is 2\n",
    "| foo|null|    2| Count when word is foo\n",
    "| foo|   1|    1| When word is foo and num is 1\n",
    "| foo|   2|    1| When word is foo and num is 2\n",
    "+----+----+-----+s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rollup() returns a subset of the rows returned by cube(). rollup returns 6 rows whereas cube returns 8 rows. Here are the missing rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+----+----+-----+\n",
    "|word| num|count|\n",
    "+----+----+-----+\n",
    "|null|   1|    1| Word is null and num is 1\n",
    "|null|   2|    3| Word is null and num is 2\n",
    "+----+----+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rollup(\"word\", \"num\") doesn’t return the counts when only word is null.\n",
    "\n",
    "Let’s switch around the order of the arguments passed to rollup and view the difference in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|word| num|count|\n",
      "+----+----+-----+\n",
      "|null|null|    4|\n",
      "|null|   1|    1|\n",
      "|null|   2|    3|\n",
      "| bar|   2|    2|\n",
      "| foo|   1|    1|\n",
      "| foo|   2|    1|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.rollup(\"num\", \"word\")\n",
    "  .count()\n",
    "  .sort(asc(\"word\"), asc(\"num\"))\n",
    "  .select(\"word\", \"num\", \"count\")\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the rows missing from rollup(\"num\", \"word\") compared to cube($\"word\", $\"num\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+----+----+-----+\n",
    "|word| num|count|\n",
    "+----+----+-----+\n",
    "| bar|null|    2| Word equals bar and num is null\n",
    "| foo|null|    2| Word equals foo and num is null\n",
    "+----+----+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rollup(\"num\", \"word\") doesn’t return the counts when only num is null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pivot__\n",
    "\n",
    "Pivots make it possible for you to convert a row into a column. For example, in our current data we\n",
    "have a Country column. With a pivot, we can aggregate according to some function for each of those\n",
    "given countries and display them in an easy-to-query way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "| Banana|  1000|    USA|\n",
      "|Carrots|  1500|    USA|\n",
      "|  Beans|  1600|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Banana|   400|  China|\n",
      "|Carrots|  1200|  China|\n",
      "|  Beans|  1500|  China|\n",
      "| Orange|  4000|  China|\n",
      "| Banana|  2000| Canada|\n",
      "|Carrots|  2000| Canada|\n",
      "|  Beans|  2000| Mexico|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: Seq[(String, Int, String)] = List((Banana,1000,USA), (Carrots,1500,USA), (Beans,1600,USA), (Orange,2000,USA), (Orange,2000,USA), (Banana,400,China), (Carrots,1200,China), (Beans,1500,China), (Orange,4000,China), (Banana,2000,Canada), (Carrots,2000,Canada), (Beans,2000,Mexico))\n",
       "df: org.apache.spark.sql.DataFrame = [Product: string, Amount: int ... 1 more field]\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq((\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"),\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"),\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"),\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\"))\n",
    "\n",
    "val df = data.toDF(\"Product\",\"Amount\",\"Country\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source : https://sparkbyexamples.com/spark/how-to-pivot-table-and-unpivot-a-spark-dataframe/\n",
    "\n",
    "Spark SQL provides pivot function to rotate the data from one column into multiple columns. It is an aggregation where one of the grouping columns values transposed into individual columns with distinct data. To get the total amount exported to each country of each product, will do group by Product, pivot by Country, and the sum of Amount.\n",
    "\n",
    "This will transpose the countries from DataFrame rows into columns and produces below output. where ever data is not present, it represents as null by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico| USA|\n",
      "+-------+------+-----+------+----+\n",
      "| Orange|  null| 4000|  null|4000|\n",
      "|  Beans|  null| 1500|  2000|1600|\n",
      "| Banana|  2000|  400|  null|1000|\n",
      "|Carrots|  2000| 1200|  null|1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pivotDF: org.apache.spark.sql.DataFrame = [Product: string, Canada: bigint ... 3 more fields]\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpivot is a reverse operation, we can achieve by rotating column values into rows values. Spark SQL doesn’t have unpivot function hence will use the stack() function. Below code converts column countries to row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "| Orange|  China| 4000|\n",
      "|  Beans|  China| 1500|\n",
      "|  Beans| Mexico| 2000|\n",
      "| Banana| Canada| 2000|\n",
      "| Banana|  China|  400|\n",
      "|Carrots| Canada| 2000|\n",
      "|Carrots|  China| 1200|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "unPivotDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Product: string, Country: string ... 1 more field]\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unPivotDF = pivotDF.select(col(\"Product\"),\n",
    "    expr(\"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"))\n",
    "    .where(\"Total is not null\")\n",
    "unPivotDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-Defined Aggregation Functions\n",
    "\n",
    "User-defined aggregation functions (UDAFs) are a way for users to define their own aggregation functions based on custom formulae or business rules. You can use UDAFs to compute custom calculations over groups of input data (as opposed to single rows). Spark maintains a single AggregationBuffer to store intermediate results for every group of input data.\n",
    "\n",
    "To create a UDAF, you must inherit from the UserDefinedAggregateFunction base class and implement the following methods:\n",
    "- inputSchema represents input arguments as a StructType\n",
    "- bufferSchema represents intermediate UDAF results as a StructType\n",
    "- dataType represents the return DataType\n",
    "- deterministic is a Boolean value that specifies whether this UDAF will return the same result for a given input\n",
    "- initialize allows you to initialize values of an aggregation buffer\n",
    "- update describes how you should update the internal buffer based on a given row\n",
    "- merge describes how two aggregation buffers should be merged\n",
    "- evaluate will generate the final result of the aggregation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
